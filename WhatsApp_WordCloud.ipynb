{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 40489\n"
     ]
    }
   ],
   "source": [
    "#reading the text file\n",
    "input_file = open('chat.txt', 'r')\n",
    "\n",
    "#taking the lines of the text file inside a list\n",
    "all_lines = list()\n",
    "for lines in input_file:\n",
    "    all_lines.append(str(lines).strip(\"\\n\"))\n",
    "    \n",
    "print(\"Length: {}\".format(len(all_lines)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "#function definition for removing emojis from a text\n",
    "def demojify(input_text):\n",
    "    output_text = \"\"\n",
    "    for character in input_text:\n",
    "        if character not in emoji.UNICODE_EMOJI:\n",
    "            output_text += character\n",
    "        else: continue\n",
    "            \n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading all_lines list and creating another list (a refined one)\n",
    "modified_all_lines = list()\n",
    "\n",
    "for i in range(1, len(all_lines)):\n",
    "    modified_all_lines.append(demojify(all_lines[i][20:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "#forming a new list of texts\n",
    "final_lines = list()\n",
    "\n",
    "for j in range(len(modified_all_lines)):\n",
    "    index = modified_all_lines[j].find(':')\n",
    "    final_lines.append(modified_all_lines[j][index+2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "#forming a list containing all words\n",
    "all_words = list()\n",
    "\n",
    "#removing urls (if any) from texts\n",
    "for i in range(len(final_lines)):\n",
    "    line = final_lines[i]\n",
    "    reg_exp = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "    if re.match(reg_exp, line):\n",
    "        final_lines[i] = re.sub(reg_exp, '', final_lines[i], flags=re.MULTILINE)\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "#tokenizing the words from the sentences of the list\n",
    "for line in final_lines:\n",
    "    for tokens in word_tokenize(line):\n",
    "        all_words.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "\n",
    "#function definition to generate a WordCloud\n",
    "def generateWordCloud(input_list):\n",
    "    comment_words = \"\"\n",
    "    for elements in input_list:\n",
    "        tokens = elements.split()\n",
    "        for i in range(len(tokens)):\n",
    "            tokens[i] = tokens[i].lower()\n",
    "        comment_words += \" \".join(tokens) + \" \"\n",
    "        \n",
    "    wordcloud = WordCloud(width=800, height=800,\n",
    "                          background_color=\"white\",\n",
    "                          min_font_size=10).generate(comment_words)\n",
    "    \n",
    "    #plotting the wordcloud image\n",
    "    plt.figure(figsize = (8, 8), facecolor = None) \n",
    "    #plt.imshow(wordcloud) \n",
    "    plt.axis(\"off\") \n",
    "    plt.tight_layout(pad = 0) \n",
    "  \n",
    "        \n",
    "#generating the WordCloud\n",
    "generateWordCloud(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
